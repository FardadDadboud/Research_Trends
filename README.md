# Research_Trends
Papers and links will be provided. 

# Image to Image Translation
Image-to-Image Translation with Conditional Adversarial Nets: https://phillipi.github.io/pix2pix/

# Interpretation

Interpretable Explanations of Black Boxes by Meaningful Perturbation: https://openaccess.thecvf.com/content_iccv_2017/html/Fong_Interpretable_Explanations_of_ICCV_2017_paper.html

Contrastive explanation:
https://github.com/IBM/Contrastive-Explanation-Method.git

# Adverserial Samples

Explaining and Harnessing Adversarial Examples: https://arxiv.org/abs/1412.6572

DeepFool: a simple and accurate method to fool deep neural networks: https://arxiv.org/abs/1511.04599

# Augmentation

mixup: Beyond Empirical Risk Minimization: https://arxiv.org/abs/1710.09412

CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features: https://arxiv.org/abs/1905.04899

# Object Detection

## General Concepts

* C4W3L01 to C4W3L10: https://www.youtube.com/watch?v=ArPaAX_PhIs&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF

## One-Stage

### YOLO Family

* YOLO V1: You Only Look Once: Unified, Real-Time Object Detection: https://arxiv.org/abs/1506.02640

* YOLO V2: YOLO9000: Better, Faster, Stronger: https://arxiv.org/abs/1612.08242

* YOLO V3: An Incremental Improvement: https://arxiv.org/abs/1804.02767

* YOLO V4: Optimal Speed and Accuracy of Object Detection: https://arxiv.org/abs/2004.10934

* YOLO V5: https://github.com/ultralytics/yolov5

* YOLOR: You Only Learn One Representation: Unified Network for Multiple Tasks: https://arxiv.org/abs/2105.04206

* YOLOX: Exceeding YOLO Series in 2021: https://arxiv.org/abs/2107.08430

### Others

* SSD: Single Shot MultiBox Detector: https://arxiv.org/abs/1512.02325

* RetinaNet: Focal Loss for Dense Object Detection:https://arxiv.org/abs/1708.02002v2

* CenterNet: Objects as Points: https://arxiv.org/abs/1904.07850

* EfficientDet: Scalable and Efficient Object Detection: https://arxiv.org/abs/1911.09070

* Swin Transformer: Hierarchical Vision Transformer using Shifted Windows: https://arxiv.org/abs/2103.14030 


## Two-Stage

### R-CNN Family

* RCNN: Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation: https://github.com/rbgirshick/rcnn

* Fast-RCNN: https://github.com/rbgirshick/fast-rcnn

* Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks: https://github.com/rbgirshick/py-faster-rcnn

* Mask-RCNN: https://arxiv.org/abs/1703.06870

### Others

* Feature Pyramid Networks for Object Detection: https://arxiv.org/abs/1612.03144v2

* R-FCN: Object Detection via Region-based Fully Convolutional Networks: https://arxiv.org/abs/1605.06409

* DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution: https://arxiv.org/abs/2006.02334

# Vision Transformers
## Review
A Survey on Vision Transformer: https://ieeexplore.ieee.org/iel7/34/9970415/09716741.pdf
## Tasks

### Object Detection

### Object Tracking

### Sensor Fusion

* An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://github.com/google-research/vision_transformer

# Unsupervised Representation Learning

## General Concepts

* **Berekely Course - Deep Unsupervised Learning - Spring 2020 - L7:** Self-Supervised Learning / Non-Generative Representation Learning: https://www.youtube.com/watch?v=dMUes74-nYY

  [![Everything Is AWESOME](https://yt-embed.herokuapp.com/embed?v=dMUes74-nYY)](https://www.youtube.com/watch?v=dMUes74-nYY "Self-Supervised Learning / Non-Generative Representation Learning")

* **Self Supervised Learning for Object Detection:** https://www.youtube.com/watch?v=q_ZI5dPPBM8
  [![Everything Is AWESOME](https://yt-embed.herokuapp.com/embed?v=q_ZI5dPPBM8)](https://www.youtube.com/watch?v=q_ZI5dPPBM8 "SSelf Supervised Learning for Object Detection")

## Contrastive Learning

### From Images

* **SimCLR:** A Simple Framework for Contrastive Learning of Visual Representations: https://github.com/google-research/simclr

* **MoCo:** Momentum Contrast for Unsupervised Visual Representation Learning: https://github.com/facebookresearch/moco

## Bootstraping

* Bootstrap your own latent (BYoL): A new approach to self-supervised Learning: https://github.com/deepmind/deepmind-research/tree/master/byol

* FLOWE: Self-Supervised Representation Learning from Flow Equivariance: https://arxiv.org/abs/2101.06553

* Emerging Properties in Self-Supervised Vision Transformers: https://github.com/facebookresearch/dino

## In the Applications

### Car Detection and Tracking

* There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge: https://arxiv.org/abs/2103.01353

### Moving Animal Detection

* Self-supervised Video Object Segmentation by Motion Grouping: https://arxiv.org/abs/2104.07658

## Others

Barlow Twins: Self-Supervised Learning via Redundancy Reduction: https://github.com/facebookresearch/barlowtwins

Representation Learning by Learning to Count: https://arxiv.org/abs/1708.06734

Self-Supervised Feature Learning by Learning to Spot Artifacts: https://arxiv.org/abs/1806.05024

Watching the World Go By: Representation Learning from Unlabeled Videos: https://arxiv.org/abs/2003.07990

Space-Time Correspondence as a Contrastive Random Walk: https://github.com/ajabri/videowalk

Unsupervised learning of object frames by dense equivariant image labelling: https://arxiv.org/abs/1706.02932

# Unclassified

Deep learning time series forcasting:
https://github.com/Alro10/deep-learning-time-series.git
