# Research_Trends
Papers and links will be provided. 

# Image to Image Translation
Image-to-Image Translation with Conditional Adversarial Nets: https://phillipi.github.io/pix2pix/

# Interpretation

Interpretable Explanations of Black Boxes by Meaningful Perturbation: https://openaccess.thecvf.com/content_iccv_2017/html/Fong_Interpretable_Explanations_of_ICCV_2017_paper.html

Contrastive explanation:
https://github.com/IBM/Contrastive-Explanation-Method.git

# Adverserial Samples

Explaining and Harnessing Adversarial Examples: https://arxiv.org/abs/1412.6572

DeepFool: a simple and accurate method to fool deep neural networks: https://arxiv.org/abs/1511.04599

# Augmentation

mixup: Beyond Empirical Risk Minimization: https://arxiv.org/abs/1710.09412

CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features: https://arxiv.org/abs/1905.04899

# Object Detection

* YOLO V1: You Only Look Once: Unified, Real-Time Object Detection: https://arxiv.org/abs/1506.02640

* YOLO V2: YOLO9000: Better, Faster, Stronger: https://arxiv.org/abs/1612.08242

* YOLO V3: An Incremental Improvement: https://arxiv.org/abs/1804.02767

* YOLO V4: Optimal Speed and Accuracy of Object Detection: https://arxiv.org/abs/2004.10934

# Unsupervised Representation Learning

## General Concepts

* **Berekely Course - Deep Unsupervised Learning - Spring 2020 - L7:** Self-Supervised Learning / Non-Generative Representation Learning: https://www.youtube.com/watch?v=dMUes74-nYY

  [![Everything Is AWESOME](https://yt-embed.herokuapp.com/embed?v=dMUes74-nYY)](https://www.youtube.com/watch?v=dMUes74-nYY "Self-Supervised Learning / Non-Generative Representation Learning")

* **Self Supervised Learning for Object Detection:** https://www.youtube.com/watch?v=q_ZI5dPPBM8
  [![Everything Is AWESOME](https://yt-embed.herokuapp.com/embed?v=q_ZI5dPPBM8)](https://www.youtube.com/watch?v=q_ZI5dPPBM8 "SSelf Supervised Learning for Object Detection")

## Contrastive Learning

### From Images

* **SimCLR:** A Simple Framework for Contrastive Learning of Visual Representations: https://github.com/google-research/simclr

* **MoCo:** Momentum Contrast for Unsupervised Visual Representation Learning: https://github.com/facebookresearch/moco

## Bootstraping

* Bootstrap your own latent (BYoL): A new approach to self-supervised Learning: https://github.com/deepmind/deepmind-research/tree/master/byol

* FLOWE: Self-Supervised Representation Learning from Flow Equivariance: https://arxiv.org/abs/2101.06553

## Others

Barlow Twins: Self-Supervised Learning via Redundancy Reduction: https://github.com/facebookresearch/barlowtwins

Representation Learning by Learning to Count: https://arxiv.org/abs/1708.06734

Self-Supervised Feature Learning by Learning to Spot Artifacts: https://arxiv.org/abs/1806.05024

Watching the World Go By: Representation Learning from Unlabeled Videos: https://arxiv.org/abs/2003.07990

Space-Time Correspondence as a Contrastive Random Walk: https://github.com/ajabri/videowalk

Unsupervised learning of object frames by dense equivariant image labelling: https://arxiv.org/abs/1706.02932

# Unclassified

Deep learning time series forcasting:
https://github.com/Alro10/deep-learning-time-series.git
